使用机器学习的方法解决这个问题
1.TF-IDF + 逻辑回归
2.PCA降维得到主成分然后+逻辑回归
3.使用kNN分类

4.使用朴素贝叶斯进行分类
step 1. 挑选出无用词，如“的”，“我”。 使用的方法是根据数据分析，用普通的文本找出这些词的频率，然后再在unlabel标签数据集上分析。

01. 应该在处理每个文本的时候，应该去除一些杂乱信息，减少内存占用等  
02. 如果在事先有词典的情况下，可以直接提取文本特征  
03. 没有词典的时候，应该自己构造词典，甚至在大量样本中学习词典。由于没有事先的词典dict，把所有文档的分词结果放到一个dictionary里面，然后根据词频从高到低排序。由于处理每个文档的时候，就没有去除一些杂乱信息，比如标点符号、无意义的数字等，所以在试验中构造最终词典(固定选取1000个词)的时候，逐渐去除词典的部分高频项，观察正确率的变化  
04. 特征维数的选取，在本文中固定1000维，可以做正确率关于维数的变化    
05. 特别说明：因为分类器用的是朴素贝叶斯，所以文本特征是[TRUE, FALSE, ...]。文本是否包含字典中词的判别p(feature_i | C_k) = ...如果是使用SVM，那么特征应该是词频或者TDIDF等  
06. 可以采用nltk或sklearn，注意其中选取的特征格式不同。nltk要求特征为dict格式，sklearn要求特征为list  

如果仅以是否出现过特征词作为该维的特征（也就是one-hot向量）有点儿过于弱了，所以能不能搞点儿别的作为特征向量。
