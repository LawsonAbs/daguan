<!--
 * @Author: LawsonAbs
 * @Date: 2021-09-15 20:49:15
 * @LastEditTime: 2021-09-15 20:49:16
 * @FilePath: /daguan/risk_data_grand/达观杯比赛.md
-->
# 达观杯比赛
## 1.任务
风险数据识别，主要应用在金融+政务。
给出的是加密数据（汉字=>数字），判断这些由数字id组成的内容存在何种风险？

## 2.数据
加密文本，包含四个文件：
- 训练集，测试集，未标注数据集

## 3.衡量指标
$f1_{macro} = \frac{1}{N} \sum_{i=1}^{n}{f_i}$

## 4.模型
01.`nezha `
02. `robeta-large-chinese`

## 5.问题关键
类别不均衡问题引起
主要解决方法：
### 5.1 `focal loss`
通过对不同的类的惩罚来改进模型对各种不同样例的学习，对于容易区分的类，则让其损失相对原损失（以交叉熵计算）变小；如果是不易区分的类，则让其损失相对原损失变大。思想如下：

- 局限
如果加上系数α，则有两个超参（α，γ）需要调整，对显卡的要求则非常高。

- 代码
```python
# 实现FocalLoss
class FocalLoss(nn.Module):
    r'''
    alpha(1D Tensor, Variable) : the scalar factor for this criterion
    gamma(float, double) : gamma > 0; reduces the relative loss for well-classiﬁed examples (p > .5), 
                            putting more focus on hard, misclassiﬁed examples
    size_average(bool): By default, the losses are averaged over observations for each minibatch.
                        However, if the field size_average is set to False, the losses are
                        instead summed for each minibatch.
    '''
    # size_average 是什么参数？
    def __init__(self, class_num,alpha=None,gamma=2,size_average=True):
        super().__init__()
        self.gamma = 2
        self.class_num = class_num
        self.size_average = size_average
        if alpha is None: # 这里的self.alpha 是torch
            self.alpha = torch.ones(class_num, 1).cuda() # size = [class_num,1]
        else:
            self.alpha = alpha

    def forward(self, inputs,targets):
        # 得到batch 和 class_num
        N = inputs.size(0) # batch_size 
        C = inputs.size(1) # class_num
        P = F.softmax(inputs) # 对输入做softmax，得到每个类别的logits
        # sf = torch.nn.Softmax()
        class_mask = inputs.data.new(N, C).fill_(0) # 按照inputs的shape创建一个初始值为0【可指定】的tensor
        # class_mask = Variable(class_mask)
        ids = targets.view(-1, 1) # 由一维变二维
        class_mask.scatter_(1, ids.data, 1.)
        #print(class_mask)

        if inputs.is_cuda and not self.alpha.is_cuda:
            self.alpha = self.alpha.cuda()
        alpha = self.alpha[ids.data.view(-1)] # 将alpha 的形状适配成inputs的 size = > (batch_size,1)

        probs = (P*class_mask).sum(1).view(-1,1)

        log_p = probs.log()
        #print('probs size= {}'.format(probs.size()))
        #print(probs)

        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p 
        #print('-----bacth_loss------')
        #print(batch_loss)

        if self.size_average:
            loss = batch_loss.mean()
        else:
            loss = batch_loss.sum()
        return loss
```
### 5.2 `label smooth`
一种trick。出发点有：
- 考虑类间的相似性。以本竞赛数据为例：
![在这里插入图片描述](https://img-blog.csdnimg.cn/06bebf3d3ed04512882c915f62177d19.png)
上述的几个类中其实都是划分为“企业”这一大类，再继续划分为“市值下跌”，“违法行为”，“行政处罚”，“产品问题”等四小类。假设现在有一个标签是`[1,0,0,0]` 即代表该类数据是 企业_市值下跌，那么我么就可以使用label_smooth 的方法将其设置成 
$$[1-\epsilon,\frac{\epsilon}{3},\frac{\epsilon}{3},\frac{\epsilon}{3}]$$
其中$\epsilon$的取值大概在`0.01-0.1` 之间。

- `label smooth` 往什么方向改变交叉熵损失？是使得整个损失变大还是变小？

代码：
```python
class LabelSmoothingLoss(nn.Module):
    """
    NLL loss with label smoothing.
    weight = t.ones[]
    """
    # 这里的smoothing 应该再往大一点儿调，0.01 太小了。=> 改到0.1
    def __init__(self, smoothing=0.1):
        super(LabelSmoothingLoss, self).__init__()
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing

    def forward(self, x, target):
        log_probs = torch.nn.functional.log_softmax(x, dim=-1)
        # step1. 得到NLLLoss，也就是CrossEntropy 计算的值
        nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(1))
        nll_loss = nll_loss.squeeze(1)

        # step2. => 取均值
        smooth_loss = -log_probs.mean(dim=-1)

        # 为什么有个self.confidence * nll_loss?
        loss = self.confidence * nll_loss + self.smoothing * smooth_loss
        return loss.mean()
```
### 5.3 `weight loss`
首要问题是：怎么计算权重？第j类样本的权重如下：
$$w_j \quad = \frac{N}{C} * N_j$$
其中N是样本总数，C是类别数，$N_j$ 是第j类的样本个数。

如果使用`CrossEntropy`，有个`weight`参数可以使用，如果使用自己写的loss，则可以在针对每个类别的时候乘一起即可。

# 总结
研究要以方法为中心，而比赛要以数据为中心。
深度学习就是研究数据分布，并找到一种比较合理模拟分布的函数，将特征映射到某一个结果上。

# 参考资料
- [苏建林 focal loss](https://kexue.fm/archives/4733)
- focal loss [论文](https://arxiv.org/abs/1708.02002)
- [nezha](https://arxiv.org/abs/1909.00204) 